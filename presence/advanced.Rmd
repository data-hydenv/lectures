---
title: "Advanced SQL Techniques"
description: "Learn in depth database structuring with SQL."
author:
  - name: "Mirko MÃ¤licke"
    url: "https://hyd.iwg.kit.edu/personen_maelicke.php"
    affiliation: "Karlsruhe Institute for Technology (KIT)"
    affiliation_url: "https://hyd.iwg.kit.edu"
date: "`r Sys.Date()`"
output:  
  html_document:
    number_sections: yes
    toc: yes
    toc_float: yes
  html_notebook:
    number_sections: yes
    toc: yes
    toc_float: yes
  radix::radix_article:
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = FALSE)
# require the package
if (!("RPostgreSQL" %in% installed.packages())){
  install.packages("RPostgreSQL")
}
if (!("getPass" %in% installed.packages())){
  install.packages("getPass")
}
if (!("dplyr" %in% installed.packages())){
  install.packages("dplyr")
}
if (!("ggplot2" %in% installed.packages())){
  install.packages("ggplot2")
}
library(RPostgreSQL)
library(getPass)
library(ggplot2)
library(dplyr)


pw <- function () {
  if (Sys.getenv('POSTGRES_PASSWORD') == ""){
    return(getPass('Provide the password: '))
  } else {
    return(Sys.getenv('POSTGRES_PASSWORD'))
  }
}

# establish the connection
drv <- dbDriver('PostgreSQL')
con <- dbConnect(drv, host='hydenv.hydrocode.de', port=5432, user='hydenv', 
                 password=pw(), dbname='hydenv')
```

# Temporarily creating objects

Especially for development and data analysis tasks it is very useful to create temporary results. This saves a lot of time and will keep
 your database clean as you do not have to remember which objects were only intermediate and can be dropped.
The SQL language knows the <span style="color:blue">TEMPORARY</span> keyword, which can be used along with <span style="color:blue">CREATE</span> statements. 
This is most often used on the creation of tables and views. The temporary tables can be used just like persistent tables, but once you close the connection used for creation, the table will automatically be dropped. This can save you from a lot of cleanup work. 

<div class="alert alert-warning">Some SQL clients open and close a connection on each command issued in order to keep the number of connection small and prevent the user from keeping open connections. You will have to change the settings or the tool in case you want to use temporary object using these tools.</div>
<hr>
The structure of our `raw_data` table is normalized. If this makes sense for the database used in the lecture is questionable and can be discussed. In productive use-cases, this is very common. 

<div class="alert alert-info">To recap: A normalized data-tabe here means, you can save any kind of numeric information into a single relation. You only have a (compound) primary key and the main value. Lookup tables are used to describe the data values and link information like location or observed variable to the base numbers.</div>

To transition normalized data from a general purpose data management system into something like an *application scheme*, there are number of possible patterns.
A common way to is to put each and every observation type into its own relation and describe it by a lookup table, which describes the mandatory metadata. This can be extended by an optional, or variable-specific second metadata lookup table.
This might seem strange, but it very flexible, easy to maintain and depending on the implementation extremely performant.
But one step after the other. 

## Creating a new structure

There are two structures that can help use here: <span style="color:blue">VIEW</span> and <span style="color:blue">TABLE</span>, both can be created only temporary.

### Explicit CREATE

Let's first have a look at the variables table to decide, which kind of data should be *un-normalized*.
```{sql connection=con}
SELECT * FROM variables
```

We will first focus first on the temporary table for temperature and light.

```{sql connection=con}
create temporary table temperature (
  meta_id integer,
  tstamp timestamp without time zone,
  value real not null,
  PRIMARY KEY (meta_id, tstamp)
);
```

Next, we need to issue an <span style="color:blue">INSERT</span> for each record we want to see in this new table. 
If you are already familiar with programming languages, you might already loop in your mind over the `raw_data` table and add the needed data one-by-one into the `temperature` data. The big advantage of relational database systems is, that they are truely vectorized and we can just specify how the data should transition, the database engine will apply our extression to the **whole** relation at once.

```{sql connection=con}
INSERT INTO temperature 
SELECT meta_id, tstamp, value FROM raw_data WHERE variable_id=1
```

```{sql connection=con}
SELECT count(*) FROM temperature
```

### Implicit CREATE

For larger tables it can be quite tedious to define each table and column by hand. Especially for temporary tables which are not persisted at all. You might have noticed, that the two new tables will not only look very alike, but also have some similarities to the `raw_data` table. Luckily, SQL is really flexible as it can create structures from query results. That means, we can select the light data from `raw_data` and create a new table **with** the data already contained in one step.

You can literally turn any query result into a table.

```{sql connection=con}
CREATE temporary TABLE light AS
SELECT meta_id, tstamp, value FROM raw_data WHERE variable_id=2
```


```{sql connection=con}
SELECT count(*) from light
```
Now, let's just load the data to R and plot it. You can change the id to your HOBO.

```{sql connection=con, output.var="df"}
SELECT * FROM temperature WHERE meta_id = 123
```
```{r}
df %>%
  ggplot(aes(x=tstamp, y=value)) + 
  geom_line(color='red', linewidth=2)
```



## Data Views

### Re-creating temporary tables

A major drawback of the creating new tables, even temporary ones, is that we effectively doubled the footprint of our data application in terms of disk space, as we stored all measurements twice. Additionally, we might run into data integrity issues in use-cases involving highly dynamic data operations. The `raw_data` might be updated, before we are finished working with the `temperature` table. Then, the `temperature` is **not** updated along, which might be the intended behavior, but is usually not.<br>
To overcome these drawbacks, we can use a structure called <span style="color:blue">VIEW</span>, which might be created persistent or temporary. A view is like a persistent <span style="color: blue">SELECT</span> that will be rerun each time you query the results. 
Here, temporary tables or views can get quite powerful. It will behave like a dynamic table with the only difference that you can't edit it. From a performance point of view, a view is not capable of indices, which can make it substantially slower than a table. On the other hand a view is just a stored query that will be executed on each query and therefore will not take any disk space and update automatically. For the HOBO data, this seems to be the more suitable solution.<br>

We can go ahead and create the views, unfortunately, we will run into an error:
```{sql connection=con}
create temporary view temperature as
select meta_id, tstamp, value as temperature from raw_data where variable_id=1
```

We can go ahead and close the connection to the database, this will drop the temporary tables. When re-opening, we should be able to create the views without any naming conflicts:

```{r}
dbDisconnect(con)
con <- dbConnect(drv, host='hydenv.hydrocode.de', port=5432, user='hydenv', 
                 password=pw(), dbname='hydenv')
```
```{sql connection=con}
create temporary view temperature as
select meta_id, tstamp, value as temperature from raw_data where variable_id=1
```

That worked without naming error so far. Let's checkout the data:
```{sql connection=con}
select * from temperature limit 5
```

### Analytical and statistical views.

So far, we used temporary tables and views only to safe us from typing a simple filter statement. Although this is already a valid justification and there are definitely use-cases, like easier data download in applications, the effort was not really worth it up to this point.
The true power of views is to persist more complex statistical reductions or even analysis workflow steps as sensible data views. These can be chained together and make development and debugging esier, or, additionally, are even a useful insight on their own.
Let's create a view on a view, for example calculating current daily indices:
```{sql connection=con}
create temporary view temperature_indices as
select meta_id, 
  date_trunc('day', tstamp) as date, 
  avg(temperature) as mean, 
  min(temperature) as min, 
  max(temperature) as max
from temperature
group by meta_id, date
```
```{sql connection=con}
select * from temperature_indices order by date desc limit 5
```

This could also be calculated for the day measurements only, by simply adding a filter.
```{sql connection=con}
create temporary view day_temperature_indices as
select meta_id, 
  date_trunc('day', tstamp) as date, 
  avg(temperature) as mean, 
  min(temperature) as min, 
  max(temperature) as max
from temperature
where date_part('hour', tstamp) <= 8 and date_part('hour', tstamp) <= 18
group by meta_id, date
```
```{sql connection=con}
select * from day_temperature_indices order by date desc limit 5
```

This is a great overview table for doing some in-depth analysis of the day or night temperature. Once you streamed your results into R, a persistent table or any kind of text-based file, you can just close the database connection and all temporary tables and views will be dropped. The next time you run the script again, the views will automatically use the updated base data.

```{sql connection=con, output.var="temperature"}
select * from temperature_indices
```
```{sql connection=con, output.var="temperature.day"}
select * from day_temperature_indices
```
```{r}
temperature %>%
  filter(date > '2020-11-01' & date < '2021-02-1') %>%
  ggplot(aes(x=date, y=mean)) + 
  geom_point(color='brown')

temperature.day %>%
  filter(date > '2020-11-01' & date < '2021-02-1') %>%
  ggplot(aes(x=date, y=mean)) + 
  geom_point(color='red', alpha=0.8)

```

Maybe combine it somehow...
```{r}
full_join(temperature, temperature.day, by='date') %>%
  filter(date > '2020-11-01' & date < '2021-02-1') %>%
  ggplot(aes(x=date, y=value)) +
  geom_point(aes(y=mean.x), color='brown', size=3, alpha=0.2) + 
  geom_point(aes(y=mean.y), color='salmon', size=1, alpha=0.3)

```

<div class="alert alert-success">Before you continue, play around with these objects a little bit. You should get a good feeling for whether to use temporary objects or not. You could for example create another view that holds the exact same information but only for night temperatures.
</div>

You should use this example to get a feeling for the interplay of R and PostgreSQL. Obviously, you could also load the full data table and then implement the reducers in R only. With libraries like `tibble` and `dplyr`, you are perfectly equipped for tasks like this. You need to find a way, that feels comfortable for you, but also think about technical implications, like processing times, network load, memory usage and scalability, which might limit you.
The other way around, it is of course also possible, to implement the last step of combining both indices tables in SQL. The concept, which is called `full_join` in dplyr is actually borrowed from SQL and will be discussed in the next section. In fact, JOINs are the most important and most fundamental concept behind **relational** database and kind of giving them their names.    

# Joins
## chaining Joins
This section will give some more insights on <span style="color:blue">JOINS</span>. It will cover different types of JOINS and how to chain a JOIN. 
Quite often you will find yourself in a situation, where you have to join a lookup table to a table of interest 
and this lookup table is itself described by another lookup table. 
Querying these structures is fairly straightforward as you can just chain joins together. 
This can be demonstrated by the vegetation_cover table

```{sql connection=con}
select * from hobo h
join vegetation_cover vc on st_within(st_transform(h.geom, 31467), vc.geom)
join vegetation_cover_description vd on vd.id=vc.description_id 
```

Now, we could join this result to the data table and make all this meta data available to every measurement record. 
This will obviusly take some time. 
In this case you are not interested in the full meta_data record but just in a subset. Of course we want to come up with a fast solution.
<br> Let's build two different Views producing the same output.

```{sql connection=con}
create temporary view join_then_filter as
select d.tstamp, d.value, h.hobo_id, v.name as variable, v.unit, vd.description as vegetation from data_extended d
join hobo h on h.hobo_id=d.hobo_id
join variables v on d.variable_id=v.id
join vegetation_cover vc on st_within(st_transform(h.geom, 31467), vc.geom)
join vegetation_cover_description vd on vd.id=vc.description_id 
where v.id=1 and h.geom is not null and year=2018
```
```{sql connection=con}
create temporary view filter_then_join as
select d.tstamp, d.value, h.hobo_id, v.name as variable, v.unit, vd.description as vegetation from data_extended d
join hobo h on h.hobo_id=d.hobo_id and h.year=2018
join variables v on d.variable_id=v.id and v.id=1
join vegetation_cover vc on st_within(st_transform(h.geom, 31467), vc.geom) and h.geom is not null
join vegetation_cover_description vd on vd.id=vc.description_id
```

Let's have a look on the first two rows of both views:
```{sql connection=con}
select * from join_then_filter limit 2
```
```{sql connection=con}
select * from filter_then_join limit 2
```

<div class="alert alert-warning">Before you continue, what do you think is the faster query and why?</div>

```{sql connection=con}
explain analyze select * from join_then_filter
```
```{sql connection=con}
explain analyze select * from filter_then_join
```

<div class="alert alert-success">Have a close look and don't focus on the total time. The query plan is exactly the same, that means although we were using a different query logic, PostgreSQL kind of got the idea behind our query and tried to find the fastest solution.</div>

## left/right join

In all the preceding examples, the join of two tables went quite well as every entry in the first table could find at least on entry 
on the second table. Switching to 'database language' here, the first table will be called the _left_ and the second one the _right_ table.
By default, the database will always join the right to the left one. If there is no foreign key referencing a record on the right table, there won't be a join for these entries. <br>
Thus, the _default_ join is in reality a <span style="color:blue">LEFT JOIN</span>. 
The oposite direction for building the join can be achieved by using a <span style="color:blue">RIGHT JOIN</span>.<br>
This can be illustrated by building an easy example.
```{sql connection=con, warning=F, echo=F, message=F}
create temporary table roles (
  id serial constraint pkey_roles primary key,
  name text
);
create temporary table people (
  id serial constraint pkey_people primary key,
  name text,
  role_id integer constraint fkey_role references roles
);
insert into roles (id, name) values (1, 'jun. developer'), (2, 'sen. developer'), (3, 'boss');
insert into people (name, role_id) values ('alex', 1), ('dave', NULL), ('christine', 3), ('brian', 2), ('melanie', 2);
```

looks like:
```{sql connection=con}
select * from people
```
```{sql connection=con}
select * from roles
```

Now, the left join should give us the expected example of the roles.id and roles.name bound to everybody except dave. The right join on the other hand should give us the roles with the people bound to it. 

<div class="alert alert-warning">Before you procede think about the following aspects:
  * What will happen to dave? Omitted?
  * Who will be bound to sen. developer - brian or melanie - and why?
</div>

```{sql connection=con}
select * from people p left join roles r on p.role_id=r.id
```
```{sql connection=con}
select * from people p right join roles r on p.role_id=r.id
```
 
Not what you expected? Well, the left/right just gives the **direction**. That's why dave is only missing in the right join.
For the opposite **behaviour** we will need another _type_ of joining, more on that in the next section. 
What you might have expected for the second example is the opposite _join_.:
```{sql connection=con}
select * from roles r join people p on p.role_id=r.id
```
with the other direction:
```{sql connection=con}
select * from roles r right join people p on p.role_id=r.id
```

## inner/outer join

The _type_ of join is controlled by the <span style="color:blue">INNER</span> and <span style="color:blue">OUTER</span> keyword.
This sets the condition on omitting and duplication of entires in the result. 
Think of these keywords from a mengenlehre perspective. The INNER subset of both tables are the ones which are described by a
foreign key on the left and a primary key on the right table (if the direction is _left_). 
The OUTER subset are the records which are either described by a **primary key** on the left or right, no matter what the direction 
might look like.
```{sql connection=con}
select * from people p inner join roles r on p.role_id=r.id
```
```{sql connection=con}
select * from people p left outer join roles r on p.role_id=r.id
```
```{sql connection=con}
delete from people where id=1
```
```{sql connection=con}
select * from people p left outer join roles r on p.role_id=r.id
```

```{sql connection=con}
select * from people p inner join roles r on p.role_id=r.id
```
<div class="alert alert-success">LEFT/RIGHT joins decide on the __direction__ you look at the joined information. If you look from the left, you will only see entries from the left table and vice versa. INNER/OUTER joins decide on the __type__ of join. This controls the conditions for including/excluding/duplicating entries in the result.</div>

<div class="alert alert-warning">There is also something called a <span style="color:blue">NATURAL JOIN</span> in PostgreSQL.
This will use the foreign key from the left table using the exactly same name as the primary key in the right table and 
omit both columns in the result.</div>


### Back to temporary objects

In fact we flooded the database with a lot of views and tables during this session. Some of them were tables of quite substatial size compared to the pre-existing tables. Do you still remember every single object? No?
No problem, these were all temporaray objects. Now close the connection:

```{r}
dbDisconnect(con)
```

Restablish:

```{r}
# establish the connection
drv <- dbDriver('PostgreSQL')
con <- dbConnect(drv, host='v45522.1blu.de', port=5432, user=getPass('Provide the user'), 
                 password=getPass('Provide the password'), dbname='datamanagement')
```

And the tables are gone:
```{r}
'temperature_indices' %in% dbListTables(con) | 'temperature' %in% dbListTables(con) | 'people' %in% dbListTables(con)
```

# cleanup
```{r}
dbDisconnect(con)
```