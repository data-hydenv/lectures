---
title: "Advanced SQL Techniques"
description: "Learn in depth database structuring with SQL."
author:
  - name: "Mirko MÃ¤licke"
    url: "https://hyd.iwg.kit.edu/personen_maelicke.php"
    affiliation: "Karlsruhe Institute for Technology (KIT)"
    affiliation_url: "https://hyd.iwg.kit.edu"
date: "`r Sys.Date()`"
output:  
  html_document:
    number_sections: yes
    toc: yes
    toc_float: yes
  html_notebook:
    number_sections: yes
    toc: yes
    toc_float: yes
  radix::radix_article:
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = FALSE)
# require the package
if (!("RPostgreSQL" %in% installed.packages())){
  install.packages("RPostgreSQL")
}
if (!("getPass" %in% installed.packages())){
  install.packages("getPass")
}
if (!("dplyr" %in% installed.packages())){
  install.packages("dplyr")
}
if (!("ggplot2" %in% installed.packages())){
  install.packages("ggplot2")
}
library(RPostgreSQL)
library(getPass)
library(ggplot2)
library(dplyr)


pw <- function () {
  if (Sys.getenv('POSTGRES_PASSWORD') == ""){
    return(getPass('Provide the password: '))
  } else {
    return(Sys.getenv('POSTGRES_PASSWORD'))
  }
}

# establish the connection
drv <- dbDriver('PostgreSQL')
con <- dbConnect(drv, host='hydenv.hydrocode.de', port=5432, user='hydenv', 
                 password=pw(), dbname='hydenv')
```

# Temporarily creating objects

Especially for development and data analysis tasks it is very useful to create temporary results. This saves a lot of time and will keep
 your database clean as you do not have to remember which objects were only intermediate and can be dropped.
The SQL language knows the <span style="color:blue">TEMPORARY</span> keyword, which can be used along with <span style="color:blue">CREATE</span> statements. 
This is most often used on the creation of tables and views. The temporary tables can be used just like persistent tables, but once you close the connection used for creation, the table will automatically be dropped. This can save you from a lot of cleanup work. 

<div class="alert alert-warning">Some SQL clients open and close a connection on each command issued in order to keep the number of connection small and prevent the user from keeping open connections. You will have to change the settings or the tool in case you want to use temporary object using these tools.</div>
<hr>
The structure of our `raw_data` table is normalized. If this makes sense for the database used in the lecture is questionable and can be discussed. In productive use-cases, this is very common. 

<div class="alert alert-info">To recap: A normalized data-tabe here means, you can save any kind of numeric information into a single relation. You only have a (compound) primary key and the main value. Lookup tables are used to describe the data values and link information like location or observed variable to the base numbers.</div>

To transition normalized data from a general purpose data management system into something like an *application scheme*, there are number of possible patterns.
A common way to is to put each and every observation type into its own relation and describe it by a lookup table, which describes the mandatory metadata. This can be extended by an optional, or variable-specific second metadata lookup table.
This might seem strange, but it very flexible, easy to maintain and depending on the implementation extremely performant.
But one step after the other. 

## Creating a new structure

There are two structures that can help use here: <span style="color:blue">VIEW</span> and <span style="color:blue">TABLE</span>, both can be created only temporary.

### Explicit CREATE

Let's first have a look at the variables table to decide, which kind of data should be *un-normalized*.
```{sql connection=con}
SELECT * FROM variables
```

We will first focus first on the temporary table for temperature and light.

```{sql connection=con}

```

Next, we need to issue an <span style="color:blue">INSERT</span> for each record we want to see in this new table. 
If you are already familiar with programming languages, you might already loop in your mind over the `raw_data` table and add the needed data one-by-one into the `temperature` data. The big advantage of relational database systems is, that they are truly vectorized and we can just specify how the data should transition, the database engine will apply our expression to the **whole** relation at once.

```{sql connection=con}

```

```{sql connection=con}
SELECT count(*) FROM temperature
```

### Implicit CREATE

For larger tables it can be quite tedious to define each table and column by hand. Especially for temporary tables which are not persisted at all. You might have noticed, that the two new tables will not only look very alike, but also have some similarities to the `raw_data` table. Luckily, SQL is really flexible as it can create structures from query results. That means, we can select the light data from `raw_data` and create a new table **with** the data already contained in one step.

You can literally turn any query result into a table.

```{sql connection=con}

```


```{sql connection=con}

```
Now, let's just load the data to R and plot it. You can change the id to your HOBO.

```{sql connection=con, output.var="df"}
SELECT * FROM temperature WHERE meta_id = 123
```
```{r}

```

## Data Views

### Re-creating temporary tables

A major drawback of the creating new tables, even temporary ones, is that we effectively doubled the footprint of our data application in terms of disk space, as we stored all measurements twice. Additionally, we might run into data integrity issues in use-cases involving highly dynamic data operations. The `raw_data` might be updated, before we are finished working with the `temperature` table. Then, the `temperature` is **not** updated along, which might be the intended behavior, but is usually not.<br>
To overcome these drawbacks, we can use a structure called <span style="color:blue">VIEW</span>, which might be created persistent or temporary. A view is like a persistent <span style="color: blue">SELECT</span> that will be rerun each time you query the results. 
Here, temporary tables or views can get quite powerful. It will behave like a dynamic table with the only difference that you can't edit it. From a performance point of view, a view is not capable of indices, which can make it substantially slower than a table. On the other hand a view is just a stored query that will be executed on each query and therefore will not take any disk space and update automatically. For the HOBO data, this seems to be the more suitable solution.<br>

We can go ahead and create the views, unfortunately, we will run into an error:
```{sql connection=con}
create temporary view temperature as
select meta_id, tstamp, value as temperature from raw_data where variable_id=1
```

We can go ahead and close the connection to the database, this will drop the temporary tables. When re-opening, we should be able to create the views without any naming conflicts:

```{r}
dbDisconnect(con)
con <- dbConnect(drv, host='hydenv.hydrocode.de', port=5432, user='hydenv', 
                 password=pw(), dbname='hydenv')
```
```{sql connection=con}
create temporary view temperature as
select meta_id, tstamp, value as temperature from raw_data where variable_id=1
```

That worked without naming error so far. Let's checkout the data:
```{sql connection=con}
select * from temperature limit 5
```

### Analytical and statistical views.

So far, we used temporary tables and views only to safe us from typing a simple filter statement. Although this is already a valid justification and there are definitely use-cases, like easier data download in applications, the effort was not really worth it up to this point.
The true power of views is to persist more complex statistical reductions or even analysis workflow steps as sensible data views. These can be chained together and make development and debugging esier, or, additionally, are even a useful insight on their own.
Let's create a view on a view, for example calculating current daily indices:
```{sql connection=con}
create temporary view temperature_indices as

```
```{sql connection=con}
select * from temperature_indices order by date desc limit 5
```

This could also be calculated for the day measurements only, by simply adding a filter.
```{sql connection=con}
create temporary view day_temperature_indices as

```
```{sql connection=con}
select * from day_temperature_indices order by date desc limit 5
```

This is a great overview table for doing some in-depth analysis of the day or night temperature. Once you streamed your results into R, a persistent table or any kind of text-based file, you can just close the database connection and all temporary tables and views will be dropped. The next time you run the script again, the views will automatically use the updated base data.

```{sql connection=con, output.var="temperature"}
select * from temperature_indices
```
```{sql connection=con, output.var="temperature.day"}
select * from day_temperature_indices
```
```{r}

```

Maybe combine it somehow...
```{r}
left_join(temperature, temperature.day, by=c('meta_id', 'date')) %>%
  filter(date > '2020-11-01' & date < '2021-02-1') %>%
  ggplot(aes(x=date, y=mean.x)) +
  geom_point(aes(y=mean.x), color='brown', size=3, alpha=0.2) + 
  geom_point(aes(y=mean.y), color='orange', size=1, alpha=0.5)

```

<div class="alert alert-success">Before you continue, play around with these objects a little bit. You should get a good feeling for whether to use temporary objects or not. You could for example create another view that holds the exact same information but only for night temperatures.
</div>

You should use this example to get a feeling for the interplay of R and PostgreSQL. Obviously, you could also load the full data table and then implement the reducers in R only. With libraries like `tibble` and `dplyr`, you are perfectly equipped for tasks like this. You need to find a way, that feels comfortable for you, but also think about technical implications, like processing times, network load, memory usage and scalability, which might limit you.
The other way around, it is of course also possible, to implement the last step of combining both indices tables in SQL. The concept, which is called `full_join` in dplyr is actually borrowed from SQL and will be discussed in the next section. In fact, JOINs are the most important and most fundamental concept behind **relational** database and kind of giving them their names.    

# Joins
## re-creating the example
This section will give some more insights on <span style="color:blue">JOINS</span>. It will cover different types of JOINS and how to use multiple joins at once.
First, we can implement the join done with R from the last example with SQL:

```{sql connection=con, output.var="combined"}

```
```{r}
View(combined)
```
```{r}
combined %>%
  ggplot(aes(x=date, y=mean.all)) +
  geom_point(aes(y=mean.all), color='brown', size=3, alpha=0.2) + 
  geom_point(aes(y=mean.day), color='orange', size=1, alpha=0.5)
```

## joining tables

Maybe you have wondered, why not all of the metadata about the hobos was visible when we worked on that table. Some of the more optional metadata is moved into yet another table called `details`. Let's have a look at that table.
```{sql connection=con}
SELECT * from details LIMIT 5
```

That looks weird. Which keys are available?
```{sql connection=con}
SELECT DISTINCT key FROM details
```

This is a quite common pattern to store arbitrary key=value pairs to extend structured metadata. We will first focus on `'exposition'`. The `metadata` table is then referenced by a third table called `nm_metadata_details`. That is called an *association table*, as it only stores pairs of references to the primary key of `metadata` and `details`. This is the way, how a N:M or many-to-many cardinality is implemented. This allows us to store **and update** common key-value combinations only once, despite of the number of references. Let's checkout details involving exposition and count how often they have been referenced.
```{sql connection=con}
select * from details where key='exposition'
```
```{sql connection=con}

```

If we would decide to use the full names, like 'North' instead of 'N', we need only one update on the `details` table to affect all references. We can do the same thing for the sunlight influence and join both to the `metadata` table.
To do that effectively, we need to introduce the concepts of *sub-queries*. There is no need to join two tables or views, we can also join a table to another <span style="color:blue">SELECT</span> in-place.
```{sql connection=con}

```

As a final step, we can use the result from above to check if there is a general difference in temperature indices between the combinations on exposition and influence.
```{sql connection=con}

```

## left/right join

In all the preceding examples, the join of two tables went quite well as every entry in the first table could find at least on entry on the second table. Switching to 'database language' here, the first table will be called the _left_ and the second one the _right_ table.
By default, the database will always join the right to the left one. If there is no foreign key referencing a record on the right table, there won't be a join for these entries. <br>
Thus, the _default_ join is in reality a <span style="color:blue">LEFT JOIN</span>. 
The oposite direction for building the join can be achieved by using a <span style="color:blue">RIGHT JOIN</span>.
<br>
To work a little bit on joins, we will introduce yet another data example. Right now (winter term 2022/2023), a collaboration of > 20 scientists from > 15 universities is working on a so-called CAMELS dataset for Germany. These are homogenized, daily resolved discharge timeseries for medium sized catchments with unified meteorological data and catchment attributes.
There are two tables involved: `camels_de_metadata` and `camels_de_data`. Please note that the data is currently being processed and can only be used for this lecture.

<div class="alert alert-warning">Please note that the CAMELS-DE dataset is not yet published. Although it will be freely available, you have access to preliminary processed data, which does **not** fall under an open license. If you load the data to your computer, you **have to** delete it after the lecture.</div>

```{sql connection=con}
SELECT * FROM camels_de_metadata LIMIT 10
```

The `camels_id` is the primary key and referenced by the data table `camels_de_data`. With a left join, we can join the metadata to the data and count the available days of data as aggregation information.
```{sql connection=con}

```

The `LEFT` can be omitted here, as every join is a left join if the direction is not further specified. The opposing one is the `RIGHT JOIN`. This specifies the *direction* from which one table is joined to the other. The final result set cannot contain any information, that has no key in the primary table. Here, that means, the `LEFT JOIN` will contain all camels_id that exist in the left (primary) table. Now we only change the direction of the join:
```{sql connection=con}

```

You can notice that the camel_ids are not continuous anymore. There are only 237 instead of 775 rows in the result set. The `count` column, which originates from the right table is now always filled, there are no NAs anymore.

<div class="alert alert-info">There is no *correct* way to join these two tables. The main question is if you want to avoid NA values in important columns or ensure integrity of your primary column (camels_id)</div>


## inner/outer join

The _type_ of join is controlled by the <span style="color:blue">INNER</span> and <span style="color:blue">OUTER</span> keyword.
The type of join decides on inclusion of filling of un-matching keys.
Think of these keywords in terms of set theory. The INNER subset of both tables are the ones which have a primary key in the left and a foreign key in the right table. 
The OUTER JOIN create the (set theory) union of both tables. To illustrate this, we will work on a limited metadata set, so that there is data without metadata and metadata without data
```{sql connection=con}
create temporary view small_meta as
select * from camels_de_metadata 
ORDER BY camels_id
LIMIT 80
```

```{sql connection=con}

```

You can see, that the <span style="color:blue">JOIN</span> is still a left one. All 80 primary keys are in the result set. 
All joins we created so far were outer joins on the left table. That is the default behavior. The other type is the <span style="color:blue">INNER JOIN</span>, which does not have a direction.

```{sql connection=con}

```


<div class="alert alert-success">LEFT/RIGHT joins decide on the __direction__ you look at the joined information. If you look from the left, you will only see entries from the left table and vice versa. INNER/OUTER joins decide on the __type__ of join. This controls the conditions for keeping complete entries in the result set.</div>

<div class="alert alert-warning">There is also something called a <span style="color:blue">NATURAL JOIN</span> in PostgreSQL.
This will use the foreign key from the left table using the exactly same name as the primary key in the right table and 
omit both columns in the result.</div>


# cleanup
```{r}
dbDisconnect(con)
```