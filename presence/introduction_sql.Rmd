---
title: "Introduction to PostgreSQL"
description: |
  Notes to introductory database lecture.
author:
  - name: Mirko MÃ¤licke
    url: https://hyd.iwg.kit.edu/personen_maelicke.php
    affiliation: Karlsruhe Institute for Technology (KIT)
    affiliation_url: https://hyd.iwg.kit.edu
date: "`r Sys.Date()`"
output:  
  html_notebook:
    number_sections: yes
    toc: yes
    toc_float: yes
  html_document:
    number_sections: yes
    toc: yes
    toc_float: yes
  radix::radix_article:
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = FALSE)
# require the package
if (!("RPostgreSQL" %in% installed.packages())){
  install.packages("RPostgreSQL")
}
if (!("getPass" %in% installed.packages())){
  install.packages("getPass")
}
require(RPostgreSQL)
require(getPass)

pw <- function () {
  if (Sys.getenv('POSTGRES_PASSWORD') == ""){
    return(getPass('Provide the password: '))
  } else {
    return(Sys.getenv('POSTGRES_PASSWORD'))
  }
}

# establish the connection
drv <- dbDriver('PostgreSQL')
con <- dbConnect(drv, host='localhost', port=5432, user='hydenv', 
                 password=pw(), dbname='hydenv')
```

# SQL language

The SQL is defined to be a own name but is more often referred to as an abbreviation of **S**tandard **Q**uery **L**anguage. (Which is officially not correct.) 
The SQL language is a very structured but yet unflexible language. Unlike R, you have to exactly specify what and how you want to manipulate or query data in the system. Trail-and-Error like invoking and calling of function won't yield any success in most cases. 
You can use SQL for all kinds of database-related work. You can define the structure of the database, the data types used or constrains your data has to fit using SQL. 
At the same time, you push and pull data from and to the database, can change the data or calculate new values using SQL. 
Whenever you are able to perform a operation in the database itself instead of loading raw data to a front-end like R and then operating on it, you will have a much better performance in most cases. 
This is especially true for data operations like aggregations which usually tend to decrease the data density. Remind that your internet connection will be the bottleneck in noumerous calculations.<br>
Another advantage is the regularization of the SQL language. There are literally dozens of different database systems. 
Only from the world of relational database systems there is MySQL, PostgreSQL, SQLite, MSSQL or Oracle to name just the most common ones. Each of this systems understands a *SQL standard* and a *SQL accent*. The standard is a subset of defined functions and keywords that each and any database system does understand. This means by learning SQL once you will be able to operate on many different systems. 
Theoretically. In practice, these standards are very restrictive and the real power of each system lays in its accent. This
is a extension to a SQL standard, which is specific to each database management system and often enough also RDBMS version. 
Therefore, learning SQL for PostgreSQL will help you to learn other systems, but unfortunatelly not prevent you from learning other accents.

# SQL Query

Any execution command pushed to the database system is called a query. This does not necessarily mean that you are actually *querying*, i.e. asking for data. The command for deleting a whole table is also called a query.
Usually you define the basic **Operation**, followed by the **Entitiy** to operate on, the specified optional **Parameters** how to run the operation and a **Filter** statement which data points shall be affected. Additionally, a **grouping** and a **sorting** can be appended to data queries. 
The four most important database operations are <span style="color:blue">SELECT</span>, <span style="color:blue">INSERT</span>, <span style="color:blue">UPDATE</span> and <span style="color:blue">DELETE</span>. The main entities we will be focusing on are tables, columns and constrains. The only filter we will use is the so called <span style="color:blue">WHERE</span>-clause. <br>

## select

Now, let's start with the 'Hello World!' example of the database world: select all rows (datasets) from one table (entity).

```{sql connection=con}
SELECT * FROM metadata
```

The '\*' means _all_ here. So instead of writing down all single columns we want to load, we can use the star. In the databse language a column is also called an *attribute*, that has different *values* for each dataset. 
Alternatively, you can specifiy all attributes, that you are interested in explicitly:

```{sql connection=con}
SELECT id, device_id, term_id FROM metadata
```

We can use the same strategy to query other tables as well. There is another table called `terms`, which does refer back to the `term_id` attribute shown above. We will learn more about this later. Let's just have a look at the `terms` table:

```{sql connection=con}
SELECT * FROM terms
```

As the `metadata` table has an attribute `term_id`, it is close, that these two things are connected. We can use this information to filter the table for HOBOs of a specific generation by using a <span style="color:blue">WHERE</span> clause. Additionally we will only ask for the id, device_id and the term_id attribute.

```{sql connection=con}
select id, device_id, term_id from metadata where term_id=9
```

As you can see, SQL is not case-sensitive. But it is convention to write SQL keyword all UPPERCASE and entities, columns and properties lowercase, although the queries still work if you do not stick to that. THe only exception is PostgreSQL, which will not accept uppercase entity names. These would have to be quoted though. 

<div class="alert alert-warning">**CAUTION:** Unlike R, SQL makes a difference between single and double quotes. Single quotes are used for values in strings, texts and character chains, while double quotes are used for quoting entity names. E.g.:<br>
WHERE "name"='Freiburg'.<br> The opposite usage will not work.</div>

## ordering and limiting the results

The select is the most important operation for us, as we usually have structured data in the database and need very specific subsets of that data for analysis. Another powerful data extraction tool is the database ability to rapidly order data and limit the output.

We will move to another dataset of all space missions ever launched. Note that the dataset is from 2020 and therefore a bit outdated.

```{sql connection=con}
SELECT * from space_raw ORDER BY datum DESC LIMIT 5
```

```{sql connection=con}
SELECT * from space_raw ORDER BY datum ASC LIMIT 5
```

In many circumstances a sort-and-limit approach might be tremendously faster than a where.

## filter

Databases are not only good at comparing filter to values, but also in searching the values itself. You can use the percentage sign in PostgreSQL as a placeholder for _any sign_. Instead for searching for an exact match, we can filter by a _string alike_.

```{sql connection=con}
SELECT * FROM space_raw WHERE location LIKE 'LC-%'
```

These are all space missions launched from locations that start with `'LC-'`, while:

```{sql connection=con}
SELECT * FROM space_raw WHERE location LIKE '%Florida%'
```

will return all missions, which location description *contains* a `'Florida'`. You can also combine two filter, ie. to get all locations starting with `'LC-'`, but only in the USA.

```{sql connection=con}
select location from space_raw where location LIKE 'LC-%' and location LIKE '%USA%'
```

Finally, you somtines only want duplicates removed, or to put it the other way around: Your result set should be **distinct**.

```{sql connection=con}
select distinct location from space_raw where location LIKE 'LC-%' and location LIKE '%USA%'
```


## aggregation

The SQL langauage is especially useful when it comes to data aggregation. SQL can not only be used to take over most of the aggregation parts in a data analysis pipeline, it is also useful to decrease the data density before sending it through your internet connection.
The easiest aggregation is to aggregate a whole table using the function _count_. 

```{sql connection=con}
select count(*) from metadata
```

```{sql connection=con}
select count(*) from space_raw
```
This is useful in case you need to know how many datasets are present in a table.<br>
Nevertheless, this is a quite unique aggregation. Usually you will aggregate in a grouping statement. You will query data, group it by any attribute and any non-grouping attribute has to be aggregated within this group.

```{sql connection=con}
select term_id, count(*) from metadata group by term_id
```

The group by statement does also work on the result of functions. We can use the _substr_ function to extract the first letter.

```{sql connection=con}
select substr(location, 1,3) as location_start, count(id) from space_raw group by substr(location, 1,3)
```

This can easily be combined with sorting:

```{sql connection=con}
select substr(location, 1,3) as location_start, count(id) from space_raw group by substr(location, 1,3) order by substr(location, 1,3) desc
```

In case we are only interested in two of the substrings entries we can follow two different strategies. Either group-aggregate-filter, or filter-union. Depending on the data structure these two queries can show clear performance differences while yielding the same results:

```{sql connection=con}
select substr(location, 1,3) as location_start, count(id) from space_raw where substr(location, 1, 3) in ('Sta', 'LC-') group by substr(location, 1,3) order by count desc
```
```{sql connection=con}
select substr(location, 1,3) as location_start, count(id) from space_raw where substr(location, 1, 3)='Sta' group by substr(location, 1,3)
UNION
Select substr(location, 1,3) as location_start, count(id) from space_raw where substr(location, 1, 3)='LC-' group by substr(location, 1,3)
```

# Joins

The one feature making PostgreSQL actually an _relational_ database system is the fact that, technically, there are no duplicates in any table. Tha means each row (or _relation_) has an unique identifier. This is called the **primary key**. 
For ``metadata, this is the _id_ attribute. 
There is also another table called *raw_data*. Let's have a look at that table:
```{sql connection=con}
SELECT * from raw_data limit 5
```

The raw_data table has a *compound* or *combined* primary key. Each row has to be unique on each combination of 'meta_id' , 'variable_id' and 'tstamp'.
One sensor can have multiple measurements and there can be multiple measurements at the same point in time. The combination has to be unique as one sensor cannot take multiple measurements at the same time.
A database system can now easily join the meta data about a sensor and the actual measurement together whenever needed. Here, `metadata` further describes the data living in `raw_data`. Or in other words, `metadata` is a **lookup table** for `raw_data`. This is their relationship (therefore **relational** database). Any relationship does also have a **cardinality** further specifying the kind of relation. in this case it is a 1:m or 'one to many' relationship as one metadata row can *hold* many raw_data entries.

This technique descreases the table size, increases performance and makes the maintainance way easier. 
On the other hand, it makes queries more complicated. The <span style="color: blue">JOIN</span> command can be used to utilize this connection.

```{sql connection=con, echo=TRUE}
select * from raw_data
  join metadata on raw_data.meta_id=metadata.id

limit 10
```

The table above summarizes all information we have available about every record of measurement. Now imagine that the hobo table might easily have 20 attributes. This demonstrates, why we should split up our data as much as possible. 
This is called **normalization**, because you not only decrease the data density, improve query performance and make your data less error prone, you will also define entities as closed structures and break complex data into small comprehensible pieces.

Using the techniques from above, we can now create summaries or indices about our data in the database, before it is transferred to our local machine.
```{sql connection=con}
select m.id, term_id, count(tstamp) as amount, avg(value) as mean_value 
from raw_data r
join metadata m on m.id=r.meta_id
group by m.id, term_id
```

But there is definitely something weird going on, as the mean values do not really make sense. The reason is, that currently the raw_data table includes **all** data, thus we have temperature and light intensity observations mixed up. This is actually a common pattern in relational databases, as it is quite flexible. Luckily, filtering is fast and straightforward in PostgreSQL. We just need to filter for the correct variable.
```{sql connection=con}

SELECT * FROM variables
```

```{sql connection=con}
select m.id, term_id, count(tstamp) as amount, avg(value) as mean_value 
from raw_data r
join metadata m on m.id=r.meta_id
WHERE variable_id = 1
group by m.id, term_id
```

This looks mch better. For the record, let's add the term name as well.
```{sql connection=con}
select m.device_id as hobo_id, t.full_name, count(tstamp) as amount, avg(value) as mean_value 
from raw_data r
join metadata m on m.id=r.meta_id
join terms t on t.id=m.term_id
WHERE variable_id = 1
group by m.device_id, t.full_name
order by hobo_id, full_name
```

This query is still considered to be relatively small and straightforward. Due to its expressive command nature, SQL queries can get quite long and nested. With this *complexity* we buy literally the fastest data processing language available. 
A query like this would still be blazing fast, if issued to a 10Mio entries table. With a little bit of configuration (and the correct hardware), PostgreSQL can handle Tera- and Peta-Bytes of data, as proved by data warehouse and data lake technologies, like Google's BigQuery.
This script already covered the most important concepts to get you started, from here it's only about learning PostgreSQL available functions.
There are a few more aggregation functions built-in and PostgreSQL offers possiblities to write custom functions in their own database language `pgsql` along with an interface to run C++ or Python functions. 
last but not least, the PostGIS extension implements a full-featured GIS system built into the database. Bye, ArcGIS.
<br>

# cleanup

After each session, you must not forget to disconnect from the database. Otherwise your connection might stay open and prevent other applications from connecting to the database.

```{r}
dbDisconnect(con)
```